# -*- coding: utf-8 -*-
"""ML_Preprocessing_and_Modeling.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1gG0awKlPikNlHB2SOVsew2HIy-02SBUI

# 1. Resume Dataset
"""

# âœ… Install dulu fitz (PyMuPDF)
!pip install PyMuPDF

# âœ… Setelah install baru import
import fitz
import pandas as pd
import numpy as np
import re
import nltk
from google.colab import files
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity

# âœ… Download stopwords
nltk.download('stopwords')
from nltk.corpus import stopwords
from nltk.stem import PorterStemmer

# âœ… Inisialisasi
stop_words = set(stopwords.words('english'))
ps = PorterStemmer()

"""## Load Dataset

Untuk keperluan proses training dan testing model, dataset yang digunakan berasal dari kaggle https://www.kaggle.com/datasets/snehaanbhawal/resume-dataset/code
"""

from google.colab import drive
drive.mount('/content/drive')

resume_df = pd.read_csv("/content/drive/MyDrive/dataset_careercompas/resume_cleaned(1).csv")

"""### Preprocessing (Clean Data)


"""

nltk.download('punkt_tab')

import nltk
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
from nltk.tokenize import word_tokenize
import re

# âœ… Download dulu data yang diperlukan
nltk.download('stopwords')
nltk.download('punkt')
nltk.download('wordnet')
nltk.download('omw-1.4')

# âœ… Inisialisasi
stop_words = set(stopwords.words('english'))
lemmatizer = WordNetLemmatizer()

# âœ… Ganti fungsi preprocess dengan lemmatization
def preprocess_text_lemmatized(text):
    if not isinstance(text, str):
        return "" # Return empty string for non-string input
    text = text.lower()
    text = re.sub(r'[^a-z\s]', ' ', text)  # ganti dengan spasi supaya tokenisasi rapi
    tokens = word_tokenize(text)
    tokens = [w for w in tokens if w not in stop_words and len(w) > 1]
    tokens = [lemmatizer.lemmatize(w) for w in tokens]
    return " ".join(tokens)


# âœ… Jalankan fungsi preprocessing
# Apply the function to the 'Resume_str' column
resume_df['clean_resume'] = resume_df['Resume_str'].apply(preprocess_text_lemmatized)

# Print the first few rows of the DataFrame with the new column
print(resume_df[['Resume_str', 'clean_resume']].head())

"""# 2. Job Dataset

Untuk keperluan proses training dan testing model, dataset yang digunakan berasal dari kaggle
https://www.kaggle.com/datasets/ravindrasinghrana/job-description-dataset

Import Data Job Description via URL CSV
"""

# URL dataset
url_jobdesc = "https://docs.google.com/spreadsheets/d/e/2PACX-1vTUzmvKziuAJBsv2g5w79DwXh9V3pOWeZPd_l3Mo2Og_GJ0LvYpVnudnoFBIXSer-sohyaQzVgZnA14/pub?output=csv"

# Load data
job_df = pd.read_csv(url_jobdesc)

print("\nðŸ’¼ Job Description Dataset")
print(job_df.head())

"""## Preprocessing (Clean Data)

Stemming dan Pembersihan Format Teks pada Dokumen untuk Analisis Teks Lebih Akurat
"""

def preprocess_job_text(text):
    if not isinstance(text, str):
        return ""
    text = text.lower()
    text = re.sub(r'\n|\r|\t', ' ', text)
    text = re.sub(r'[^a-z\s]', ' ', text)
    text = re.sub(r'\s+', ' ', text).strip()
    tokens = word_tokenize(text)
    tokens = [w for w in tokens if w not in stop_words and len(w) > 1]
    tokens = [lemmatizer.lemmatize(w) for w in tokens]
    return " ".join(tokens)

"""### Pengecekan dan Penanganan Missing Values pada Kolom 'Job Description"""

print("Missing values pada Job Description dataset:")
print(job_df['Job Description'].isnull().sum())

# Drop baris dengan missing pada kolom utama
job_df = job_df.dropna(subset=['Job Description'])

"""### Menampilkan Daftar Kolom pada Dataset Lowongan Pekerjaan"""

print("Kolom Job Description dataset:", job_df.columns)

"""### Pengecekan Nilai yang Hilang (Missing Values) pada Dataset Resume dan Job Description"""

print("Missing values pada Resume Job Description :")
print(job_df.isnull().sum())

"""### Menghitung Jumlah Baris yang Mengandung Missing Values di Dataset Job Description"""

print("Jumlah baris dengan missing values di Job Description:", job_df.isnull().any(axis=1).sum())

"""###Identifikasi Data Tidak Lengkap pada DataFrame"""

print(job_df[job_df.isnull().any(axis=1)])

"""###Menampilkan Tipe Data pada Setiap Kolom dalam DataFrame"""

print(job_df.dtypes)

"""### Mengetahui Jumlah Data Duplikat pada Dataset"""

print("Duplikat di Job Desc:", job_df.duplicated().sum())

"""### Proses Deteksi dan Penghapusan Duplikat pada Dataset Job Description"""

# Cek jumlah duplikat (sudah kamu tahu: 10)
print("Duplikat di Job Desc sebelum dihapus:", job_df.duplicated().sum())

# Hapus duplikat di Job Description dataset
job_df.drop_duplicates(inplace=True)

# Cek ulang setelah hapus
print("Duplikat di Job Desc setelah dihapus:", job_df.duplicated().sum())

# Jangan lupa reset index supaya rapi
job_df.reset_index(drop=True, inplace=True)

# Tampilkan jumlah data setelah penghapusan
print("Jumlah data Job Description setelah hapus duplikat:", len(job_df))

"""### Menyimpan Dataset Job Description yang Sudah Dibersihkan ke File CSV"""

job_df.to_csv('jobdesc_clean.csv', index=False)

print("âœ… File 'jobdesc_clean.csv' berhasil disimpan.")

"""# 3. Feature Extraction (TF-IDF)"""

def get_top_k_similar_jobs(resume_text, job_df, k=5):
    from sklearn.feature_extraction.text import TfidfVectorizer
    from sklearn.metrics.pairwise import cosine_similarity

    # Gabungkan teks penting dari job description
    job_df['combined_text'] = (
        job_df['Job Description'] + ' ' +
        job_df['skills'] + ' ' +
        job_df['Responsibilities'] + ' ' +
        job_df['Job Title'] + ' ' +
        job_df['Role']
    )

    # Preprocessing kolom combined_text
    job_df['clean_text'] = job_df['combined_text'].apply(preprocess_job_text)

    # Gabungkan semua job + 1 resume
    all_texts = job_df['clean_text'].tolist() + [resume_text]

    # TF-IDF
    vectorizer = TfidfVectorizer()
    tfidf_matrix = vectorizer.fit_transform(all_texts)

    cv_vector = tfidf_matrix[-1]
    job_vectors = tfidf_matrix[:-1]

    similarities = cosine_similarity(cv_vector, job_vectors).flatten()

    top_indices = similarities.argsort()[-k:][::-1]

    return top_indices, similarities

# Ambil satu resume [Uji Coba]
resume_text = resume_df['clean_resume'].iloc[0]

# Panggil fungsi
top_indices, similarities = get_top_k_similar_jobs(resume_text, job_df, k=20)

# Tampilkan hasil
print("\nTop 20 Job Descriptions based on similarity to CV:")
for i in top_indices:
    print(f"Job Title: {job_df.loc[i, 'Job Title']} - Similarity Score: {similarities[i]:.4f}")

"""# 4. Model (TensorFlow)

1. Persiapan Dataset
"""

import pandas as pd
import random


pairs = []

for idx, resume_text in enumerate(resume_df['clean_resume']):
    top_indices, _ = get_top_k_similar_jobs(resume_text, job_df, k=5)

    positive_pairs = [(resume_text, job_df.loc[i, 'clean_text'], 1) for i in top_indices]

    negative_indices = random.sample(
        [i for i in range(len(job_df)) if i not in top_indices],
        len(top_indices)
    )
    negative_pairs = [(resume_text, job_df.loc[i, 'clean_text'], 0) for i in negative_indices]

    pairs.extend(positive_pairs + negative_pairs)

random.shuffle(pairs)
df_pairs = pd.DataFrame(pairs, columns=['resume', 'job_desc', 'label'])

""" 2. Tokenisasi dan Padding"""

from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences

# Gabungkan semua teks
texts = df_pairs['resume'].tolist() + df_pairs['job_desc'].tolist()

tokenizer = Tokenizer(num_words=10000, oov_token="<OOV>")
tokenizer.fit_on_texts(texts)

# Tokenisasi dan padding
max_len = 200
resume_seq = tokenizer.texts_to_sequences(df_pairs['resume'])
job_seq = tokenizer.texts_to_sequences(df_pairs['job_desc'])

resume_pad = pad_sequences(resume_seq, maxlen=max_len, padding='post')
job_pad = pad_sequences(job_seq, maxlen=max_len, padding='post')

labels = df_pairs['label'].values

"""3. Bangun Model TensorFlow"""

import tensorflow as tf
from tensorflow.keras import layers, Model

input_resume = layers.Input(shape=(max_len,))
input_job = layers.Input(shape=(max_len,))

embedding = layers.Embedding(input_dim=10000, output_dim=64)

resume_embed = embedding(input_resume)
job_embed = embedding(input_job)

# BiLSTM bisa diganti dengan CNN/GRU
encoded_resume = layers.Bidirectional(layers.LSTM(64))(resume_embed)
encoded_job = layers.Bidirectional(layers.LSTM(64))(job_embed)

# Gabungkan kedua representasi
merged = layers.concatenate([encoded_resume, encoded_job])
dense1 = layers.Dense(64, activation='relu')(merged)
output = layers.Dense(1, activation='sigmoid')(dense1)

model = Model(inputs=[input_resume, input_job], outputs=output)
model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

model.summary()

"""4. Latih Model"""

model.fit(
    [resume_pad, job_pad],
    labels,
    batch_size=16,
    epochs=10,
    validation_split=0.2
)

"""5. Simpan Model"""

model.save("jobmatch_model.h5")

import pickle

with open("tokenizer.pkl", "wb") as f:
    pickle.dump(tokenizer, f)