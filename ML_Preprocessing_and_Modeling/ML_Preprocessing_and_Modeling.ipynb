{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Resume Dataset"
      ],
      "metadata": {
        "id": "SrQEO488dXKY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PZclouJMl74k",
        "outputId": "43e964c0-ddcd-4bda-98c4-fae02306595a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: PyMuPDF in /usr/local/lib/python3.11/dist-packages (1.26.0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "# ✅ Install dulu fitz (PyMuPDF)\n",
        "!pip install PyMuPDF\n",
        "\n",
        "# ✅ Setelah install baru import\n",
        "import fitz\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import nltk\n",
        "from google.colab import files\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# ✅ Download stopwords\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "# ✅ Inisialisasi\n",
        "stop_words = set(stopwords.words('english'))\n",
        "ps = PorterStemmer()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load Dataset"
      ],
      "metadata": {
        "id": "992NASgld5u-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Untuk keperluan proses training dan testing model, dataset yang digunakan berasal dari kaggle https://www.kaggle.com/datasets/snehaanbhawal/resume-dataset/code"
      ],
      "metadata": {
        "id": "eKZ23auyd-eQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e1l2UFiDk2aP",
        "outputId": "3f485334-3935-4af5-f7e1-d84dfb5ac1d1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "resume_df = pd.read_csv(\"/content/drive/MyDrive/dataset_careercompas/resume_cleaned(1).csv\")"
      ],
      "metadata": {
        "id": "fgavg9_IjllF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Preprocessing (Clean Data)\n",
        "\n"
      ],
      "metadata": {
        "id": "V7rRoftepOgR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt_tab')\n",
        "\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "import re\n",
        "\n",
        "# ✅ Download dulu data yang diperlukan\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "\n",
        "# ✅ Inisialisasi\n",
        "stop_words = set(stopwords.words('english'))\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# ✅ Ganti fungsi preprocess dengan lemmatization\n",
        "def preprocess_text_lemmatized(text):\n",
        "    if not isinstance(text, str):\n",
        "        return \"\" # Return empty string for non-string input\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'[^a-z\\s]', ' ', text)  # ganti dengan spasi supaya tokenisasi rapi\n",
        "    tokens = word_tokenize(text)\n",
        "    tokens = [w for w in tokens if w not in stop_words and len(w) > 1]\n",
        "    tokens = [lemmatizer.lemmatize(w) for w in tokens]\n",
        "    return \" \".join(tokens)\n",
        "\n",
        "\n",
        "# ✅ Jalankan fungsi preprocessing\n",
        "# Apply the function to the 'Resume_str' column\n",
        "resume_df['clean_resume'] = resume_df['Resume_str'].apply(preprocess_text_lemmatized)\n",
        "\n",
        "# Print the first few rows of the DataFrame with the new column\n",
        "print(resume_df[['Resume_str', 'clean_resume']].head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cs82XD1enSEW",
        "outputId": "3628684e-0c78-4093-8763-96a8df27ef86"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                          Resume_str  \\\n",
            "0           HR ADMINISTRATOR/MARKETING ASSOCIATE\\...   \n",
            "1           HR SPECIALIST, US HR OPERATIONS      ...   \n",
            "2           HR DIRECTOR       Summary      Over 2...   \n",
            "3           HR SPECIALIST       Summary    Dedica...   \n",
            "4           HR MANAGER         Skill Highlights  ...   \n",
            "\n",
            "                                        clean_resume  \n",
            "0  hr administrator marketing associate hr admini...  \n",
            "1  hr specialist u hr operation summary versatile...  \n",
            "2  hr director summary year experience recruiting...  \n",
            "3  hr specialist summary dedicated driven dynamic...  \n",
            "4  hr manager skill highlight hr skill hr departm...  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Job Dataset\n",
        "\n",
        "Untuk keperluan proses training dan testing model, dataset yang digunakan berasal dari kaggle\n",
        "https://www.kaggle.com/datasets/ravindrasinghrana/job-description-dataset\n",
        "\n",
        "Import Data Job Description via URL CSV"
      ],
      "metadata": {
        "id": "JOO9lWBipmDg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# URL dataset\n",
        "url_jobdesc = \"https://docs.google.com/spreadsheets/d/e/2PACX-1vTUzmvKziuAJBsv2g5w79DwXh9V3pOWeZPd_l3Mo2Og_GJ0LvYpVnudnoFBIXSer-sohyaQzVgZnA14/pub?output=csv\"\n",
        "\n",
        "# Load data\n",
        "job_df = pd.read_csv(url_jobdesc)\n",
        "\n",
        "print(\"\\n💼 Job Description Dataset\")\n",
        "print(job_df.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lfIT-yR5pmza",
        "outputId": "7c197b58-9011-4af5-e0ff-37df6566d76d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "💼 Job Description Dataset\n",
            "                      Job Title                       Role  \\\n",
            "0  Digital Marketing Specialist       Social Media Manager   \n",
            "1                 Web Developer     Frontend Web Developer   \n",
            "2            Operations Manager    Quality Control Manager   \n",
            "3              Network Engineer  Wireless Network Engineer   \n",
            "4                 Event Manager         Conference Manager   \n",
            "\n",
            "                                     Job Description  \\\n",
            "0  Social Media Managers oversee an organizations...   \n",
            "1  Frontend Web Developers design and implement u...   \n",
            "2  Quality Control Managers establish and enforce...   \n",
            "3  Wireless Network Engineers design, implement, ...   \n",
            "4  A Conference Manager coordinates and manages c...   \n",
            "\n",
            "                                              skills  \\\n",
            "0  Social media platforms (e.g., Facebook, Twitte...   \n",
            "1  HTML, CSS, JavaScript Frontend frameworks (e.g...   \n",
            "2  Quality control processes and methodologies St...   \n",
            "3  Wireless network design and architecture Wi-Fi...   \n",
            "4  Event planning Conference logistics Budget man...   \n",
            "\n",
            "                                    Responsibilities  \\\n",
            "0  Manage and grow social media accounts, create ...   \n",
            "1  Design and code user interfaces for websites, ...   \n",
            "2  Establish and enforce quality control standard...   \n",
            "3  Design, configure, and optimize wireless netwo...   \n",
            "4  Specialize in conference and convention planni...   \n",
            "\n",
            "                            Company  \n",
            "0                 Icahn Enterprises  \n",
            "1      PNC Financial Services Group  \n",
            "2  United Services Automobile Assn.  \n",
            "3                              Hess  \n",
            "4                      Cairn Energy  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preprocessing (Clean Data)\n",
        "\n",
        "Stemming dan Pembersihan Format Teks pada Dokumen untuk Analisis Teks Lebih Akurat"
      ],
      "metadata": {
        "id": "O3p7QksZppGR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_job_text(text):\n",
        "    if not isinstance(text, str):\n",
        "        return \"\"\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'\\n|\\r|\\t', ' ', text)\n",
        "    text = re.sub(r'[^a-z\\s]', ' ', text)\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "    tokens = word_tokenize(text)\n",
        "    tokens = [w for w in tokens if w not in stop_words and len(w) > 1]\n",
        "    tokens = [lemmatizer.lemmatize(w) for w in tokens]\n",
        "    return \" \".join(tokens)"
      ],
      "metadata": {
        "id": "6EEe0uJsqhyS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Pengecekan dan Penanganan Missing Values pada Kolom 'Job Description"
      ],
      "metadata": {
        "id": "V4fmknDoqt7j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Missing values pada Job Description dataset:\")\n",
        "print(job_df['Job Description'].isnull().sum())\n",
        "\n",
        "# Drop baris dengan missing pada kolom utama\n",
        "job_df = job_df.dropna(subset=['Job Description'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fp-bKUmKqwx2",
        "outputId": "83732b07-7e29-41f8-ce1a-d0000b55af59"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Missing values pada Job Description dataset:\n",
            "0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Menampilkan Daftar Kolom pada Dataset Lowongan Pekerjaan"
      ],
      "metadata": {
        "id": "ogAf84yGq4K-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Kolom Job Description dataset:\", job_df.columns)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hhatOehyq6-q",
        "outputId": "fe9ce21a-9bdf-498b-c0d6-365105f36abd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Kolom Job Description dataset: Index(['Job Title', 'Role', 'Job Description', 'skills', 'Responsibilities',\n",
            "       'Company'],\n",
            "      dtype='object')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Pengecekan Nilai yang Hilang (Missing Values) pada Dataset Resume dan Job Description"
      ],
      "metadata": {
        "id": "gN_vw9H3sSh3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Missing values pada Resume Job Description :\")\n",
        "print(job_df.isnull().sum())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "StOVjN_fq_7U",
        "outputId": "a117734b-e26f-474e-979c-6fca0b6f590b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Missing values pada Resume Job Description :\n",
            "Job Title           0\n",
            "Role                0\n",
            "Job Description     0\n",
            "skills              0\n",
            "Responsibilities    0\n",
            "Company             0\n",
            "dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Menghitung Jumlah Baris yang Mengandung Missing Values di Dataset Job Description"
      ],
      "metadata": {
        "id": "MYSv0JK7sX2a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Jumlah baris dengan missing values di Job Description:\", job_df.isnull().any(axis=1).sum())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dbcechV9rCgd",
        "outputId": "65c3a63b-348f-4b11-b24f-e56512bb1d71"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Jumlah baris dengan missing values di Job Description: 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Identifikasi Data Tidak Lengkap pada DataFrame"
      ],
      "metadata": {
        "id": "XQrYgNsvAyWP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(job_df[job_df.isnull().any(axis=1)])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "km_id-2UrrmQ",
        "outputId": "28ae0a55-cc8c-416f-c5d2-3c74432f68f2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Empty DataFrame\n",
            "Columns: [Job Title, Role, Job Description, skills, Responsibilities, Company]\n",
            "Index: []\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Menampilkan Tipe Data pada Setiap Kolom dalam DataFrame"
      ],
      "metadata": {
        "id": "GhMeWeMNA2Kw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(job_df.dtypes)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QPIJHFcPrtl3",
        "outputId": "c6b6f964-0d14-4a4e-8de8-cae8e8bf7cf0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Job Title           object\n",
            "Role                object\n",
            "Job Description     object\n",
            "skills              object\n",
            "Responsibilities    object\n",
            "Company             object\n",
            "dtype: object\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Mengetahui Jumlah Data Duplikat pada Dataset"
      ],
      "metadata": {
        "id": "aLwpfcaGBHCm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Duplikat di Job Desc:\", job_df.duplicated().sum())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N3APJNfSrvnQ",
        "outputId": "1cc1501c-40ea-4845-f965-82ac46bfd5e1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Duplikat di Job Desc: 10\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Proses Deteksi dan Penghapusan Duplikat pada Dataset Job Description"
      ],
      "metadata": {
        "id": "82a0rxZvry6N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cek jumlah duplikat (sudah kamu tahu: 10)\n",
        "print(\"Duplikat di Job Desc sebelum dihapus:\", job_df.duplicated().sum())\n",
        "\n",
        "# Hapus duplikat di Job Description dataset\n",
        "job_df.drop_duplicates(inplace=True)\n",
        "\n",
        "# Cek ulang setelah hapus\n",
        "print(\"Duplikat di Job Desc setelah dihapus:\", job_df.duplicated().sum())\n",
        "\n",
        "# Jangan lupa reset index supaya rapi\n",
        "job_df.reset_index(drop=True, inplace=True)\n",
        "\n",
        "# Tampilkan jumlah data setelah penghapusan\n",
        "print(\"Jumlah data Job Description setelah hapus duplikat:\", len(job_df))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nEGj3h3Krz4b",
        "outputId": "71acbd68-4f53-4d92-87b4-cab99e835ac7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Duplikat di Job Desc sebelum dihapus: 10\n",
            "Duplikat di Job Desc setelah dihapus: 0\n",
            "Jumlah data Job Description setelah hapus duplikat: 1989\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Menyimpan Dataset Job Description yang Sudah Dibersihkan ke File CSV"
      ],
      "metadata": {
        "id": "QRaI8lRdr4DH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "job_df.to_csv('jobdesc_clean.csv', index=False)\n",
        "\n",
        "print(\"✅ File 'jobdesc_clean.csv' berhasil disimpan.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7XJ-jQfir46o",
        "outputId": "20963db7-4eb6-4c27-c4b5-ff877c752fa6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File 'jobdesc_clean.csv' berhasil disimpan.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Feature Extraction (TF-IDF)"
      ],
      "metadata": {
        "id": "ZFV3fejBr7fe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_top_k_similar_jobs(resume_text, job_df, k=5):\n",
        "    from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "    from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "    # Gabungkan teks penting dari job description\n",
        "    job_df['combined_text'] = (\n",
        "        job_df['Job Description'] + ' ' +\n",
        "        job_df['skills'] + ' ' +\n",
        "        job_df['Responsibilities'] + ' ' +\n",
        "        job_df['Job Title'] + ' ' +\n",
        "        job_df['Role']\n",
        "    )\n",
        "\n",
        "    # Preprocessing kolom combined_text\n",
        "    job_df['clean_text'] = job_df['combined_text'].apply(preprocess_job_text)\n",
        "\n",
        "    # Gabungkan semua job + 1 resume\n",
        "    all_texts = job_df['clean_text'].tolist() + [resume_text]\n",
        "\n",
        "    # TF-IDF\n",
        "    vectorizer = TfidfVectorizer()\n",
        "    tfidf_matrix = vectorizer.fit_transform(all_texts)\n",
        "\n",
        "    cv_vector = tfidf_matrix[-1]\n",
        "    job_vectors = tfidf_matrix[:-1]\n",
        "\n",
        "    similarities = cosine_similarity(cv_vector, job_vectors).flatten()\n",
        "\n",
        "    top_indices = similarities.argsort()[-k:][::-1]\n",
        "\n",
        "    return top_indices, similarities"
      ],
      "metadata": {
        "id": "E5EfV9okUleu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Ambil satu resume [Uji Coba]\n",
        "resume_text = resume_df['clean_resume'].iloc[0]\n",
        "\n",
        "# Panggil fungsi\n",
        "top_indices, similarities = get_top_k_similar_jobs(resume_text, job_df, k=20)\n",
        "\n",
        "# Tampilkan hasil\n",
        "print(\"\\nTop 20 Job Descriptions based on similarity to CV:\")\n",
        "for i in top_indices:\n",
        "    print(f\"Job Title: {job_df.loc[i, 'Job Title']} - Similarity Score: {similarities[i]:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i0Qz7u3bsXOz",
        "outputId": "19b0e25c-ce55-4dad-e0bc-8d6d415114f1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Top 20 Job Descriptions based on similarity to CV:\n",
            "Job Title: Customer Service Manager - Similarity Score: 0.2572\n",
            "Job Title: Customer Service Manager - Similarity Score: 0.2572\n",
            "Job Title: Customer Service Manager - Similarity Score: 0.2572\n",
            "Job Title: Customer Service Manager - Similarity Score: 0.2572\n",
            "Job Title: Customer Service Manager - Similarity Score: 0.2572\n",
            "Job Title: Customer Service Manager - Similarity Score: 0.2572\n",
            "Job Title: Customer Service Manager - Similarity Score: 0.2572\n",
            "Job Title: Customer Service Manager - Similarity Score: 0.2572\n",
            "Job Title: Customer Success Manager - Similarity Score: 0.2469\n",
            "Job Title: Customer Success Manager - Similarity Score: 0.2469\n",
            "Job Title: Customer Success Manager - Similarity Score: 0.2469\n",
            "Job Title: Customer Success Manager - Similarity Score: 0.2469\n",
            "Job Title: Customer Success Manager - Similarity Score: 0.2469\n",
            "Job Title: Customer Success Manager - Similarity Score: 0.2469\n",
            "Job Title: Customer Success Manager - Similarity Score: 0.2469\n",
            "Job Title: Customer Success Manager - Similarity Score: 0.2469\n",
            "Job Title: Customer Service Manager - Similarity Score: 0.2253\n",
            "Job Title: Customer Service Manager - Similarity Score: 0.2253\n",
            "Job Title: Customer Service Manager - Similarity Score: 0.2253\n",
            "Job Title: Customer Service Manager - Similarity Score: 0.2253\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Model (TensorFlow)"
      ],
      "metadata": {
        "id": "cRh2UuqpN9VD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Persiapan Dataset"
      ],
      "metadata": {
        "id": "BV0a5-Nld27S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import random\n",
        "\n",
        "\n",
        "pairs = []\n",
        "\n",
        "for idx, resume_text in enumerate(resume_df['clean_resume']):\n",
        "    top_indices, _ = get_top_k_similar_jobs(resume_text, job_df, k=5)\n",
        "\n",
        "    positive_pairs = [(resume_text, job_df.loc[i, 'clean_text'], 1) for i in top_indices]\n",
        "\n",
        "    negative_indices = random.sample(\n",
        "        [i for i in range(len(job_df)) if i not in top_indices],\n",
        "        len(top_indices)\n",
        "    )\n",
        "    negative_pairs = [(resume_text, job_df.loc[i, 'clean_text'], 0) for i in negative_indices]\n",
        "\n",
        "    pairs.extend(positive_pairs + negative_pairs)\n",
        "\n",
        "random.shuffle(pairs)\n",
        "df_pairs = pd.DataFrame(pairs, columns=['resume', 'job_desc', 'label'])\n"
      ],
      "metadata": {
        "id": "3NXSXBo5dyNS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " 2. Tokenisasi dan Padding"
      ],
      "metadata": {
        "id": "QAuVcouyd3oX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# Gabungkan semua teks\n",
        "texts = df_pairs['resume'].tolist() + df_pairs['job_desc'].tolist()\n",
        "\n",
        "tokenizer = Tokenizer(num_words=10000, oov_token=\"<OOV>\")\n",
        "tokenizer.fit_on_texts(texts)\n",
        "\n",
        "# Tokenisasi dan padding\n",
        "max_len = 200\n",
        "resume_seq = tokenizer.texts_to_sequences(df_pairs['resume'])\n",
        "job_seq = tokenizer.texts_to_sequences(df_pairs['job_desc'])\n",
        "\n",
        "resume_pad = pad_sequences(resume_seq, maxlen=max_len, padding='post')\n",
        "job_pad = pad_sequences(job_seq, maxlen=max_len, padding='post')\n",
        "\n",
        "labels = df_pairs['label'].values"
      ],
      "metadata": {
        "id": "9v2-OvBhd0jv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Bangun Model TensorFlow"
      ],
      "metadata": {
        "id": "pxKyFBaVd564"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, Model\n",
        "\n",
        "input_resume = layers.Input(shape=(max_len,))\n",
        "input_job = layers.Input(shape=(max_len,))\n",
        "\n",
        "embedding = layers.Embedding(input_dim=10000, output_dim=64)\n",
        "\n",
        "resume_embed = embedding(input_resume)\n",
        "job_embed = embedding(input_job)\n",
        "\n",
        "# BiLSTM bisa diganti dengan CNN/GRU\n",
        "encoded_resume = layers.Bidirectional(layers.LSTM(64))(resume_embed)\n",
        "encoded_job = layers.Bidirectional(layers.LSTM(64))(job_embed)\n",
        "\n",
        "# Gabungkan kedua representasi\n",
        "merged = layers.concatenate([encoded_resume, encoded_job])\n",
        "dense1 = layers.Dense(64, activation='relu')(merged)\n",
        "output = layers.Dense(1, activation='sigmoid')(dense1)\n",
        "\n",
        "model = Model(inputs=[input_resume, input_job], outputs=output)\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 516
        },
        "id": "Ce8Mw5YSd6NK",
        "outputId": "481aff70-cc88-4227-c21d-65d1e7fd9aff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"functional\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m       Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to          \u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
              "│ input_layer (\u001b[38;5;33mInputLayer\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m200\u001b[0m)            │              \u001b[38;5;34m0\u001b[0m │ -                      │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ input_layer_1             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m200\u001b[0m)            │              \u001b[38;5;34m0\u001b[0m │ -                      │\n",
              "│ (\u001b[38;5;33mInputLayer\u001b[0m)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ embedding (\u001b[38;5;33mEmbedding\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m200\u001b[0m, \u001b[38;5;34m64\u001b[0m)        │        \u001b[38;5;34m640,000\u001b[0m │ input_layer[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],     │\n",
              "│                           │                        │                │ input_layer_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ bidirectional             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │         \u001b[38;5;34m66,048\u001b[0m │ embedding[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        │\n",
              "│ (\u001b[38;5;33mBidirectional\u001b[0m)           │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ bidirectional_1           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │         \u001b[38;5;34m66,048\u001b[0m │ embedding[\u001b[38;5;34m1\u001b[0m][\u001b[38;5;34m0\u001b[0m]        │\n",
              "│ (\u001b[38;5;33mBidirectional\u001b[0m)           │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ concatenate (\u001b[38;5;33mConcatenate\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │              \u001b[38;5;34m0\u001b[0m │ bidirectional[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],   │\n",
              "│                           │                        │                │ bidirectional_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ dense (\u001b[38;5;33mDense\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │         \u001b[38;5;34m16,448\u001b[0m │ concatenate[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │             \u001b[38;5;34m65\u001b[0m │ dense[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]            │\n",
              "└───────────────────────────┴────────────────────────┴────────────────┴────────────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)              </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">        Param # </span>┃<span style=\"font-weight: bold\"> Connected to           </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
              "│ input_layer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">200</span>)            │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                      │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ input_layer_1             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">200</span>)            │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                      │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ embedding (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">200</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        │        <span style=\"color: #00af00; text-decoration-color: #00af00\">640,000</span> │ input_layer[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],     │\n",
              "│                           │                        │                │ input_layer_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ bidirectional             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │         <span style=\"color: #00af00; text-decoration-color: #00af00\">66,048</span> │ embedding[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>)           │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ bidirectional_1           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │         <span style=\"color: #00af00; text-decoration-color: #00af00\">66,048</span> │ embedding[<span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>)           │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ concatenate (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ bidirectional[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],   │\n",
              "│                           │                        │                │ bidirectional_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">16,448</span> │ concatenate[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │             <span style=\"color: #00af00; text-decoration-color: #00af00\">65</span> │ dense[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]            │\n",
              "└───────────────────────────┴────────────────────────┴────────────────┴────────────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m788,609\u001b[0m (3.01 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">788,609</span> (3.01 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m788,609\u001b[0m (3.01 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">788,609</span> (3.01 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Latih Model"
      ],
      "metadata": {
        "id": "xEj7t6LKeCA5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(\n",
        "    [resume_pad, job_pad],\n",
        "    labels,\n",
        "    batch_size=16,\n",
        "    epochs=10,\n",
        "    validation_split=0.2\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yWeWECwXeBh6",
        "outputId": "05206c58-01b0-4ba2-c159-338b2aafeced"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m1242/1242\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m526s\u001b[0m 416ms/step - accuracy: 0.7281 - loss: 0.5179 - val_accuracy: 0.8010 - val_loss: 0.4204\n",
            "Epoch 2/10\n",
            "\u001b[1m1242/1242\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m557s\u001b[0m 413ms/step - accuracy: 0.8320 - loss: 0.3716 - val_accuracy: 0.8502 - val_loss: 0.3470\n",
            "Epoch 3/10\n",
            "\u001b[1m1242/1242\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m515s\u001b[0m 415ms/step - accuracy: 0.8784 - loss: 0.2894 - val_accuracy: 0.8830 - val_loss: 0.2932\n",
            "Epoch 4/10\n",
            "\u001b[1m1242/1242\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m565s\u001b[0m 417ms/step - accuracy: 0.9135 - loss: 0.2211 - val_accuracy: 0.8993 - val_loss: 0.2804\n",
            "Epoch 5/10\n",
            "\u001b[1m1242/1242\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m559s\u001b[0m 415ms/step - accuracy: 0.9331 - loss: 0.1795 - val_accuracy: 0.9066 - val_loss: 0.2621\n",
            "Epoch 6/10\n",
            "\u001b[1m1242/1242\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m561s\u001b[0m 414ms/step - accuracy: 0.9464 - loss: 0.1463 - val_accuracy: 0.9156 - val_loss: 0.2582\n",
            "Epoch 7/10\n",
            "\u001b[1m1242/1242\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m565s\u001b[0m 416ms/step - accuracy: 0.9554 - loss: 0.1227 - val_accuracy: 0.9134 - val_loss: 0.2588\n",
            "Epoch 8/10\n",
            "\u001b[1m1242/1242\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m516s\u001b[0m 415ms/step - accuracy: 0.9644 - loss: 0.0994 - val_accuracy: 0.9178 - val_loss: 0.2692\n",
            "Epoch 9/10\n",
            "\u001b[1m1242/1242\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m545s\u001b[0m 401ms/step - accuracy: 0.9682 - loss: 0.0924 - val_accuracy: 0.9166 - val_loss: 0.2907\n",
            "Epoch 10/10\n",
            "\u001b[1m1242/1242\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m519s\u001b[0m 415ms/step - accuracy: 0.9682 - loss: 0.0808 - val_accuracy: 0.9209 - val_loss: 0.2734\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7c56d12c3e50>"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. Simpan Model"
      ],
      "metadata": {
        "id": "L5aMEFRFsKB4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.save(\"jobmatch_model.h5\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "663v8tluTAdR",
        "outputId": "fcff2315-ecdc-4dfd-e64d-a7dfce1d776e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "\n",
        "with open(\"tokenizer.pkl\", \"wb\") as f:\n",
        "    pickle.dump(tokenizer, f)"
      ],
      "metadata": {
        "id": "OIqqNjwJTFll"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}